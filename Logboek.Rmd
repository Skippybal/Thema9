---
title: "Logboek"
author: "Kasper Notebomer"
date: "9/14/2021"
output:
  bookdown::pdf_document2:
    toc: true
    number_sections: true
    extra_dependencies: ["flafter", "tabularx"]
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Setup of the libraries.
```{r}
# Libraries
library("ggplot2")
library("kableExtra")
library("factoextra")
library("cowplot")
library("gridExtra")
library("dplyr")
library("scales")
library(ggpubr)
library(ggfortify)
library(tidyr)
library(purrr)
library(ggcorrplot)
library(reshape2)
library(e1071) 
library(Hmisc)
library(forcats)
```

<!-- * Copyright (c) 2018 Kasper Notebomer. -->
<!-- * Licensed under GPLv3. See gpl.md -->



# Introduction
The data was provided by Tsjerk Wassenaar of the RuG. The data set contains data on membrane composition and characteristics. The data set is not publicly available. The data set also does not have a publication linked to it.

Seeing as there is no paper linked to the data set I can only guess using the information that I did get that the data was gathered by making a membrane using specific variables and the measuring the resulting membranes to get variables like the thickness and compressibility.
The data set contains 14 variables and 2843 different measurement. The goal is to use measurement to predict the composition of the membrane. 
Typically you would use independent variables to predict one dependent variable. In this data set this doesn't seem to be the case, seeing as the variables that are given as parameters are seen as class variables, this means that these would be considered the labels. The parameters are: Temperature, Sterol type, Sterol concentration, Other (phospho)lipids in membrane, Aliphatic tails, Saturation index, Phosphatidyl choline concentration and Ethanol concentration. So the biggest question that the EDA should answer is which of these variables is most interesting to use as the label, or could we even predict multiple of them using the given variables.

# Data exploration

## Data reading & codebook
To start off the data is read in using read.csv(). The data frame that is created by this step is turned into a tibble and then used to create a codebook. The codebook is a csv file that contains the abbreviation, type and the description of every variable in the data set.
```{r Data reading & structure}
# Read the data and transform it to a tibble
data <- read.csv("data/dataFrame_all_sims.csv", 
                 sep = ",", header=TRUE, 
                 stringsAsFactors=FALSE)
data <- as_tibble(data)
```

Here, the codebook is made of all of the abbreviations in the data frame, their meaning and their type.
```{r Codebook}
# Create the codebook
codebook <- data.frame("Abbreviation" = colnames(data), "Type" = sapply(data, typeof), 
                       "Class" = sapply(data, class),
                       "Desciption" = c("Temperature (Kelvin)",
                                        "Sterol type", 
                                        "Sterol concentration (%)",
                                        "Other (phospho)lipids in membrane (headgroup)",
                                        "Aliphatic tails",
                                        "Saturation index (double bonds per tail)", 
                                        paste("Phosphatidyl choline concentration",
                                              "(% of non-sterol lipids)"), 
                                        "Ethanol concentration (% of solvent)",
                                        "Area per lipid (nm^2)", 
                                        "Thickness (nm)", "Bending rigidity (kB T)",
                                        "Tilt angle (degrees)", 
                                        "Z-order", "Compressibility (cN / m)"),
                       "Units" = c("Kelvin",
                                   "", 
                                   "%",
                                   "",
                                   "",
                                   "", 
                                   "", 
                                   "",
                                   "nm^2", 
                                   "nm",
                                   "kB T",
                                   "degrees", 
                                   "", 
                                   "cN / m"))
# Turn dataframe into a tibble
codebook <- as_tibble(codebook)
kable(codebook, caption = "Codebook") %>%
  kable_styling(latex_options = c("scale_down", "hold_position"))

# Write codebook to .csv file
write.csv(codebook, "data/codebook.csv", row.names = FALSE)
```
The codebook, shown in table 1, seems to have all of the correct data and datatypes. To easily access the description based on the abbreviation the following function is used.
```{r Codebook function}
# Get the description by the abbreviation from the codebook
get_des_by_ab <- function(df, abbreviation){
  description <- df[df$Abbreviation == abbreviation,]$Desciption
  return(description)
}
```


## Structure & summary


Here I'll take a look at the structure and the five number summary of the data to look for any problems or discrepancies. 

```{r Structure}
# Print the structure of the data
data.frame(variable = names(data),
           classe = sapply(data, class),
           first_values = sapply(data, function(x) paste0(head(x),  collapse = ", ")),
           row.names = NULL) %>% 
  kable(caption="Data structure")  %>%
  kable_styling(latex_options = c("scale_down", "hold_position"))

```
Looking at the structure of the data, shown in table 2, it seems like all of the columns have been read and have the right datatype.

```{r Five number summary}
# Get the five number summary of the selected columns
sum <- summary(data[c("temperature","sterol.conc","satur.index",
                      "PC.conc","ethanol.conc","APL", "thickness", "bending",
                      "tilt", "zorder","compress")])

sum <- sub(".*:", "", sum)
sum[is.na(sum)] <- 0
rownames(sum) <- c("Minimum", "Q1", "Median", "Mean", "Q3", "Maximum", "Number of NA's")

# Print five number summary using kable
kable(sum, caption="Five number summary") %>%
  kable_styling(latex_options = c("scale_down", "hold_position"))
```
Looking at the structure there doesn't seem to be immediate problem with the data, like the data being read as the wrong datatype. When looking at the five number summary, depicted in table 3, there do seem to be some rows that are missing values like the APL, thickness, bending and tilt values. These rows will be omitted for plotting and other research seeing as they can't be used.
The maximum of the bending rigidity and tilt angle seem to be too high, this probably means that these are outliers seeing as the rest of the values in the summary seem to be believable. To check this we'll make a histogram and a boxplot of these variables.

```{r "Omit NA's"}
# Remove NA's
row.has.na <- apply(data, 1, function(x){any(is.na(x))})
nona_data <- na.omit(data)
row.has.na.2 <- apply(nona_data, 1, function(x){any(is.na(x))})
paste("Number of NA's before: ", sum(row.has.na),
", Number of NA's after:", sum(row.has.na.2), sep="")
```
`r sum(row.has.na) - sum(row.has.na.2)` lines with NA's where omitted from the data set.

Here is a boxplor of all of the variables to check for any outliers.
```{r Boxplot}
nona_data %>% 
  select("temperature","sterol.conc","satur.index",
                      "PC.conc","ethanol.conc","APL", "thickness", "bending",
                      "tilt", "zorder","compress") %>%
  pivot_longer(., cols = c("temperature","sterol.conc","satur.index",
                      "PC.conc","ethanol.conc","APL", "thickness", "bending",
                      "tilt", "zorder","compress"), names_to = "Variable", values_to = "Value") %>%
  ggplot(aes(x = Variable, y = Value)) +
  geom_boxplot() + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
  ggtitle("Figure 0: Histogram of the variables")
```

The boxplot clearly shows that there are indeed a lot of outliers to be found in the dataset, especially in the bending rigidity, compressibility and tilt ange columns. Outliers are shown as black dots in the plot. Some variables also have very clean distributions, this is mostly because some variables are numerical but only contain a few different levels because of the experiment that they where gathered from.


```{r Histograms}
# Create 4 histograms
p <- ggplot(nona_data, aes(x= bending)) + 
  geom_histogram(binwidth=5) +
  ggtitle("Bending rigidity histogram")
p2 <- ggplot(nona_data, aes(x=tilt)) + 
  geom_histogram(binwidth=5) +
  ggtitle("Tilt angle histogram")
p3 <- ggplot(nona_data, aes(x= log10(bending + 1))) + 
  geom_histogram(binwidth=0.1) +
  ggtitle("Log10 bending rigidity histogram")
p4 <- ggplot(nona_data, aes(x=log10(tilt + 1))) + 
  geom_histogram(binwidth=0.1) +
  ggtitle("Log10 tilt angle histogram")
plots <- ggarrange(p, p2, p3, p4,
                   labels = c("A", "B", "C", "D"),
                   ncol = 2, nrow = 2)
title <- expression(atop(bold("Figure 1:"), 
                         scriptstyle("Histograms of bending rifidity and tilt angle")))
annotate_figure(plots,
                top=text_grob(title))
```
Both the bending and the tilt variable seem to have a lot of outliers when looking at figure 1. Even when transformed using log10 they still seem to be quite skewed. This is something we need to keep in mind in future research.


# Exploring relations between variables
## Density plot

I will now make density plots of some of the variables to see whether or not they look promising in finding class distinctions. These density plots will also make any skewing in the data apparent.
```{r Density}
# Create density plots
p <- ggplot(nona_data, aes(x=bending, colour = factor(sterol.type))) + 
  geom_density() +   
  xlab(get_des_by_ab(codebook, "bending")) + 
  ylab("Density") +
  ggtitle("Bending rigidity density plot") + 
  labs(colour = get_des_by_ab(codebook, "sterol.type"))

p2 <- ggplot(nona_data, aes(x=APL, colour = factor(sterol.type))) + 
  geom_density() +
  xlab(get_des_by_ab(codebook, "APL")) + 
  ylab("Density") +
  ggtitle("Area per lipid density plot") + 
  labs(colour = get_des_by_ab(codebook, "sterol.type"))

p3 <- ggplot(nona_data, aes(x=compress, colour = factor(sterol.type))) + 
  geom_density() +
  xlab(get_des_by_ab(codebook, "compress")) + 
  ylab("Density") +
  ggtitle("Compressibility density plot") + 
  labs(colour = get_des_by_ab(codebook, "sterol.type"))

p4 <- ggplot(nona_data, aes(x=APL, colour = factor(tails))) + 
  geom_density() +   
  xlab(get_des_by_ab(codebook, "APL")) + 
  ylab("Density") +
  ggtitle("APL density") + 
  labs(colour = get_des_by_ab(codebook, "tails"))
```

```{r Density plot arrangement, fig.cap="Density plots of bending rigidity, APL and compressability"}
# Plot density plots
plots <- ggarrange(p, p2, p3, p4,
                   labels = c("A", "B", "C", "D"),
                   ncol = 2, nrow = 2)
title <- expression(atop(bold("Figure 2:"), 
                         scriptstyle(paste("Density plots of multiple",
                                           " variables coloured on sterol type"))))
annotate_figure(plots,
                top=text_grob(title))
```
Plot A in figure 2 show that there is basically no distinction between cholesterol and ergosterol based on the bending rigidity. However, distinction between no sterol or a sterol does seem to be possible based on the bending rigidity seeing as the peaks of there classes only overlaps a small bit. There does seem to be an odd peak in the no sterol class at around 50 kB/t bending rigidity.

In plot B it looks like there is absolutely no way of distinguishing different classes based on the area per lipid, seeing as the peaks are basically in the same place. But just to be certain we'll still plot in against some other variables to make sure, seeing as it might be a very useful variable when paired with something like bending rigidity.
The results depicted in plot C also don't look very promising seeing as every peak is around the same place again.
Plot D on the other hand seems a lot more interesting seeing as all of the peaks are in slightly different locations, the DO, PO and the PI seem to overlap a lot but the DP and DI tails seem to have little overlap with the rest. It seems like APL could possibly be used as a variable to distinguish between aliphatic tails when paired with another variable.

When looking at all of the plots it looks like most of the data is skewed in one way or another, even though when we looked at the structure there didn't seem to be much. This can probably be explained by the extra dimension that was added in these plots, the sterol type and the aliphatic tails.


## Scatter plot
```{r Pairs, eval=FALSE}
# Code not run
# Results are too big and too clumpy for the analysis
pairs(nona_data,
      panel = "panel.smooth",
      pch = 20, 
      cex = 0.35,
      col = rgb(0.1, 0.4, 1, alpha = 0.4))
```
This code is for a paired scatter plot. The eval is currently set to false seeing as there are a lot of variables that are compared to each other in this data set. Running this code will result in a big plot that isn't very well interpretable in a pdf file.

First we'll plot the bending area per lipid against the rigidity of the membrane, colouring the points based on the sterol type, to look for any kind of clustering or correlation. To make it even cleared we'll also make a second plot with a loess regression. The reason we use loess regression instead of linear regression is because loess will show us a better trend line, seeing as it isn't trying to fit a straight line like linear regression. 

```{r APL Bending plot}
#Plot APL against bending rigidity
chart <- ggplot(nona_data, aes(APL, bending, colour = factor(sterol.type) )) + 
  geom_point(alpha=1/10, size=0.5) + 
  xlab(get_des_by_ab(codebook, "APL")) + 
  ylab(get_des_by_ab(codebook, "bending")) + 
  theme(axis.title.y = element_text(size=8)) + 
  ggtitle("APL against bending rigidity scatter plot") + 
  scale_color_manual(name=get_des_by_ab(codebook, "sterol.type"), 
                     labels = c("Cholesterol", 
                                "Ergosterol", 
                                "No sterol"),                        
                     values = hue_pal()(3))

plots <- ggarrange(chart,
                   chart + 
                     geom_smooth(method = "loess") + ggtitle(""),
                   labels = c("A", "B"),
                   ncol = 1, nrow = 2)

title <- expression(atop(bold("Figure 3:"), 
                         scriptstyle(paste("APL against bending rigidity,", 
                                           " with and without loess regression"))))
# Give the plot a number
annotate_figure(plots,
                top=text_grob(title))
```
Looking at the plots in figure 3 we can see that there is a clear separation in the area per lipid to bending rigidity ratio when comparing no sterol present to both cholesterol and ergosterol. When comparing cholesterol and ergosterol we can't see such a clear separation, there seems to be a lot of overlap. There are also a few outliers that are barely visible at area per lipid 0.4 and bending rigidity 0.
The correlation seem to inverse logarithmic, so to check this I'll make a plot where the bending rigidity is log transformed.

```{r Log10 APL bending plot}
# Create plot
chart <- ggplot(nona_data, aes(APL, log10(bending + 1), colour = factor(sterol.type) )) + 
  geom_point(alpha=1/10, size=0.5) + 
  xlab(get_des_by_ab(codebook, "APL")) + 
  ylab(paste( "Log10 ", get_des_by_ab(codebook, "bending"))) + 
  geom_smooth(method = "loess") + 
  ggtitle("APL against log10 bending rigidity scatter plot") + 
  scale_color_manual(name=get_des_by_ab(codebook, "sterol.type"), 
                     labels = c("Cholesterol", 
                                "Ergosterol", 
                                "No sterol"),                        
                     values = hue_pal()(3))

# Plot the Plot and add a tag
chart + labs(tag = "Figure 4: APL against log transformed bending regidity") +
      theme(plot.title = element_text(hjust = 0.5),
          plot.margin = margin(t = 10, r = 10, b = 40, l = 10),
          plot.tag.position = c(0.4, -0.1)
          )
```
Looking at figure 4 we can see that there indeed seem to be an almost linear correlation between the log10 transformed bending rigidity and the area per lipid. There does still seem to be a sleight curve in all of the trend lines when the area per lipid increases.

Here I'll plot the temperature against the bending rigidity while colouring based on saturation index, and deciding the shape based on the sterol type.
```{r Temp x bening regidity}
# Create rectangles to fit over the plot
rects <- data.frame(xstart = seq(0,600,300), xend = seq(300,900,300), col = letters[1:3])

# Create the plot
chart <- ggplot(nona_data, aes(temperature, bending, colour = factor(satur.index),
                               shape = factor(sterol.type) )) + 
  geom_jitter(alpha=1/10) +   
  xlab(get_des_by_ab(codebook, "temperature")) + 
  ylab(get_des_by_ab(codebook, "bending"))  + 
  ggtitle("Temperature against bending regidity, 
          coloured by saturation index, shaped on sterol type") + 
  theme(plot.title = element_text(size=10)) + 
  labs(colour = get_des_by_ab(codebook, "sterol.type"),
       shape = get_des_by_ab(codebook, "satur.index"))

# Print the plot and add 2 boxes for the 2 groups in the temperature variable 
chart + 
  geom_rect(aes(xmin = -Inf, xmax = 312.9, ymin = -Inf, ymax = Inf), 
            color="orange",
            fill = "green", alpha = 0.0001) + 
  geom_rect(aes(xmin = 313.1, xmax = Inf, ymin = -Inf, ymax = Inf), 
            color="purple",
            fill = "yellow", alpha = 0.0001) +
  labs(tag = "Figure 5: Temperature against bending regidity") +
  theme(plot.title = element_text(hjust = 0.5),
        plot.margin = margin(t = 10, r = 10, b = 40, l = 10),
        plot.tag.position = c(0.35, -0.1)
        )
```
When looking at the plot in figure 5, there seem to be a clear separation between a saturation index of 0 while the rest of the values seem to be clustered quite close together. There does seem to be a small separation between 0.5, 1 and 2 values. An interesting observation is that there don't seem to be any data point with a saturation index of more than 0.5 in the 328 kelvin group (the purple box).

Here the APL will be plot against the bending rigidity again, but here the colour is decided based on the saturation index and the shape on the sterol type.
```{r}
# Create plots
chart2 <- ggplot(nona_data, aes(APL, bending, colour = factor(satur.index), shape = factor(sterol.type) )) + 
  geom_jitter(alpha=1, size=0.5) +  
  xlab(get_des_by_ab(codebook, "APL")) + 
  ylab(get_des_by_ab(codebook, "bending"))  + 
  ggtitle("APL against bending regidity, 
          coloured on saturation index, shaped on sterol type") + 
  theme(plot.title = element_text(size=10))
chart2 + labs(colour = get_des_by_ab(codebook, "satur.index"), shape = get_des_by_ab(codebook, "sterol.type")) +
  labs(tag = "Figure 6: APL against bending rigidity coloured on saturation index") +
  theme(plot.title = element_text(hjust = 0.5),
        plot.margin = margin(t = 10, r = 10, b = 40, l = 10),
        plot.tag.position = c(0.4, -0.1)
        )
```
In figure 6 we can see the same kind of correlation as before, but now we can see that there also seem to be some kind of clustering based on the saturation index.

Here is a plot that shows the compressibility plotted against the bending rigidity.
```{r bending compress plot}
# Create plot
chart <- ggplot(nona_data, aes(compress, bending, colour = factor(sterol.type) )) +
  geom_point(alpha=2/10, size=0.5) +
  xlab(get_des_by_ab(codebook, "compress")) +
  ylab(get_des_by_ab(codebook, "bending")) +
  theme(axis.title.y = element_text(size=8)) +
  ggtitle("Compressibility against bending rigidity,
          coloured on sterol type") +
  scale_color_manual(name=get_des_by_ab(codebook, "sterol.type"), 
                     labels = c("Cholesterol", 
                                "Ergosterol", 
                                "No sterol"),                        
                     values = hue_pal()(3)) +
  theme(plot.title = element_text(size=10))

chart2 <- ggplot(nona_data, aes(compress, bending, colour = factor(sterol.type) )) +
  geom_point(alpha=2/10, size=0.5) +
  xlab(get_des_by_ab(codebook, "compress")) +
  ylab(get_des_by_ab(codebook, "bending")) +
  theme(axis.title.y = element_text(size=8)) +
  ggtitle("Compressibility against bending rigidity,
          coloured on sterol type, zoomed in") +
  scale_color_manual(name=get_des_by_ab(codebook, "sterol.type"), 
                     labels = c("Cholesterol", 
                                "Ergosterol", 
                                "No sterol"),                        
                     values = hue_pal()(3)) +
  scale_x_continuous(limits=c(0,120)) +
  scale_y_continuous(limits=c(0, 45))  +
  theme(plot.title = element_text(size=10))

# Arrange plots
plots <- ggarrange(chart,
                   chart2,
                   labels = c("A", "B"),
                   ncol = 1, nrow = 2) 

title <- expression(atop(bold("Figure 7:"), 
                         scriptstyle("Compressibility against bending rigidity, full and zoomed")))
annotate_figure(plots,
                top=text_grob(title))
```
In plot A of figure 7 we can't really make out much of a pattern in the dense clout to the left, but when looking at the rest of the data points there does seem to be some grouping based on whether or not no sterol or a sterol is present. When zooming in in plot B we can now see the same kind of grouping happen again based on either a sterol present or no sterol present.



## Heatmap

Here a heatmap is made to look for correlation between variables.
```{r Heatmap independent variables}
# Get correlation
correlation <- nona_data[,9:14]
melted_cormat <- melt(cor(correlation))

ggplot(data = melted_cormat, aes(x=Var1, y=Var2, fill=value)) + 
  geom_tile() + 
  ggtitle("Heatmap of the variables") +
  labs(tag = "Figure 8: Heatmap of the independent varaibles") +
  theme(plot.title = element_text(hjust = 0.5),
        plot.margin = margin(t = 10, r = 10, b = 40, l = 10),
        plot.tag.position = c(0.5, -0.1)
        )
```
The results shown in figure 8 are very interesting. It seems like all of the variables seem to be quite heavily correlated, except for APL, which is negatively correlated. It also seem like the comprehensibility is slightly less correlated then the rest of the variables. But thickness, bending, tilt and z order all seem to be heavily correlated.

## PCA
Here we'll make a PCA plot to look for possible clusters.
```{r PCA}
# PCA plot results
pca_res <- prcomp(correlation, scale. = TRUE, center = TRUE)
print(pca_res)

# Create PCA plot
plots <- ggarrange(autoplot(pca_res, data = nona_data,
                            colour = 'sterol.type', alpha=2/10, 
                            size = 5/10) + 
                     ggtitle("PCA plot") +
                     scale_color_manual(name=get_des_by_ab(codebook, "sterol.type"), 
                                        labels = c("Cholesterol", 
                                                   "Ergosterol", 
                                                   "No sterol"),                        
                                        values = hue_pal()(3)),
                   autoplot(pca_res, data = nona_data, 
                            colour = 'sterol.type', alpha=1/5, 
                            size = 1/2) + 
                     ggtitle("PCA plot zoomed in") + 
                     scale_color_manual(name=get_des_by_ab(codebook, "sterol.type"), 
                                        labels = c("Cholesterol", 
                                                   "Ergosterol", 
                                                   "No sterol"),                        
                                        values = hue_pal()(3)) +
                     scale_x_continuous(limits=c(-0.03,0.025)) +
                     scale_y_continuous(limits=c(-0.03,0.04)),
                   labels = c("A", "B"),
                   ncol = 1, nrow = 2)
# Add title
title <- expression(atop(bold("Figure 9:"), 
                         scriptstyle("PCA plot using autoplot")))
annotate_figure(plots,
                top=text_grob(title))
```
In plot A of figure 9 we can hardly see any clusters at all, it does seem to have a dense cloud to the right. Plot B cuts off the data point on the side and zooms in on the dense cloud of dots visible in plot A, a total of 271 dots aren't visible because of this. In this plot we can't really make out much of a pattern or cluster. It does, however, seem like no sterol cluster is sleightly seperated from the other sterol types.

Here is another way to make a PCA plot that is a bit more advanced seeing as it will also give us a way to identify correlation based on variables in the clustering.
```{r PCA2}
# Create PCA plot
res.pca <- prcomp(correlation, scale = TRUE)
plot1 <- fviz_eig(res.pca) + 
  theme(axis.title.y = element_text(size=6))
plot2 <- fviz_pca_var(res.pca,
                      col.var = "contrib", # Colour by contributions to the PC
                      gradient.cols = c("orange", "purple"),
                      repel = TRUE     # Avoid text overlapping
                      ) 
plot3 <- fviz_pca_biplot(res.pca, repel = FALSE,
                         col.var = "purple", # Variables colour
                         col.ind = nona_data$sterol.type,  # Individuals colour
                         label = FALSE,
                         addEllipses =  TRUE,
                         ellipse.type = "t"
                         )
# Arrange plots
plot <- arrangeGrob(plot1,  plot2,
                    plot3,
                    ncol = 2, nrow = 2, 
                    layout_matrix = rbind(c(1,2), c(3,3)))

plot <- as_ggplot(plot) +
  draw_plot_label(label = c("A", "B", "C"), size = 15,
                  x = c(0, 0.5, 0), y = c(1, 1, 0.5))
title <- expression(atop(bold("Figure 10:"), 
                         scriptstyle("PCA results using fviz_eig")))
annotate_figure(plot,
                top=text_grob(title))
```
The PCA plots, B and C, shown in figure 10 confirms what we could already see in the heat map, that is that most of the variables seem to be heavily correlated, seeing as they are pointing in the same direction, except for APL and compress. We can also see that thickness and z order are correlated just like tilt and bending. The PCA plot also still looks the same, only now it has as ellipse for clusters that was calculated using a t test. The ellipses all seem to overlap so there doesn't seem to be much clustering. 


## K means 
Here we'll use K-means clustering as a different technique to find possible clusters.
```{r K-means}
# Calculate K-means, set seed for reproducibility
set.seed(666)
km.res <- kmeans(scale(correlation), 3, nstart = 25)

# Dimension reduction using PCA
res.pca <- prcomp(correlation,  scale = TRUE)
ind.coord <- as.data.frame(get_pca_ind(res.pca)$coord)
ind.coord$cluster <- factor(km.res$cluster)
ind.coord$sterol.type <- nona_data$sterol.type
kable(head(ind.coord), caption="kmeans results") %>%
  kable_styling(latex_options = c("scale_down", "hold_position"))
```
The results in table 4 are printed to give a quick overview of the results of the K-means and to check for any possible problems. The row with sterol type is added so we can look whether or not this has anything to do with the found clusters.

```{r K-means2}
# Percentage of variance explained by dimensions
eigenvalue <- round(get_eigenvalue(res.pca), 1)
variance.percent <- eigenvalue$variance.percent
kable(head(eigenvalue), caption="Variance explained by dimensions") %>%
  kable_styling(latex_options = c("scale_down", "hold_position"))
```
In table 5 we can see the amount of variance is explained by ever dimension. Just like in the previous PCA plot we can see that over 80% of the variance is explained by a single variable, which is odd, to say the least.
```{r K-means3}
# Make pca plot based on K-means clusters
ggscatter(
  ind.coord, x = "Dim.1", y = "Dim.2", 
  color = "cluster", palette = "npg", ellipse = TRUE, ellipse.type = "convex",
  shape = "sterol.type", size = 1.5,  legend = "right", ggtheme = theme_bw(),
  xlab = paste0("Dim 1 (", variance.percent[1], "% )" ),
  ylab = paste0("Dim 2 (", variance.percent[2], "% )" )
) +
  stat_mean(aes(color = cluster), size = 4) +
  labs(tag = "Figure 11: PCA plot based on K-means clustering",
       shape = get_des_by_ab(codebook, "sterol.type")) +
  theme(plot.title = element_text(hjust = 0.5),
        plot.margin = margin(t = 10, r = 10, b = 40, l = 10),
        plot.tag.position = c(0.46, -0.1)
        )
```
This plot shown in figure 11 still doesn't seem very promising seeing as there seem to be three clear clusters, but this seems to be more because of the large spread of the data. The clusters also seem to have nothing to do with the sterol type of the membrane.

# Research question
Is it possible to use machine learning to reliably predicts the sterol type in a membrane with a higher than 80% accuracy, given the area per lipid, the bending rigidity and the compressibility?

# Data Cleaning
Here is a quick code snippet to show us the skewness of the data. This will be useful when deciding whether or not to use standardization or normalization on the columns.
```{r Skew}
apply(nona_data[c("temperature","sterol.conc","satur.index",
                      "PC.conc","ethanol.conc","APL", "thickness", "bending",
                      "tilt", "zorder","compress")], 2, skewness, na.rm =TRUE)
```
A lot of the data seems to be extremely skewed. Lets plot it in histograms to get a better look.
```{r SkewHist}
hist.data.frame(nona_data[c("temperature","sterol.conc","satur.index",
                            "PC.conc","ethanol.conc","APL", "thickness", "bending",
                            "tilt", "zorder","compress")])
```

The data seems to be skewed a lot in almost every column. If we were planning on using a machine learning algorithms that are based on the assumption that the data has a Gaussian distribution we would need to use normalization (transformation) to force the data in to such a distribution. To achieve this we could use a log or cube root transformation. We probably won't be using any algorithms that assume that our distribution is Gaussian, which is why we won't make any transformations at first.
Distributions like bending, tilt and compress have very long tails due to outliers. These could be log transformed. However they do not follow a power law distribution (they aren't fat tailed, the tail is just long), and values do not span across multiple orders of magnitude. This is why they won't be log transformed.
Seeing as most of the distributions aren't Gaussian we should use min_max normalization(scaling). This does lessen/suppress the impact of outliers, it also isn't as effective with outliers. We could also use z score normalization (standardization), but this assumes that our distributions are Gaussian, which they are not. It could still be used. However, due to the heavy skew and the high amount of outliers in some column, the scale probably won't be balanced. We could also look at box cox transformation, but I wasn't taught this method, so I'm not confident about using is. The reason we use normalization is because we might want to use and algorithm like k nearest neighbor(KNN), which is an algorithm that is very sensitive to data scale.

```{r Scaling}
scale_min_max <- function(x) (x - min(x)) / (max(x) - min(x))
nona_copy <- nona_data[c("APL", "thickness", "bending",
                                                "tilt", "zorder","compress")]

scaled_data <- as.data.frame(lapply(nona_copy, FUN=scale_min_max))
hist.data.frame(scaled_data)

# scaled_data <- as.data.frame(lapply(nona_copy, FUN=scale))
# hist.data.frame(scaled_data)

apply(scaled_data, 2, skewness, na.rm =TRUE)
```
After min max scaling we can see that the distributions stay the same but scale on all of the variables now ranges from zero to one.
In the future we could always try different forms of normalization and transformation to see whether or not our model improves.
Now the only thing left to do is add the sterol type label.
```{r Label}
scaled_data$sterol.type <- nona_data$sterol.type
kable(head(scaled_data), caption="Cleaned data") %>%
  kable_styling(latex_options = c("hold_position"), full_width = TRUE)

# Write new data to csv
write.csv(scaled_data, "data/clean_data.csv", row.names = FALSE)
```
Only the sterol type was added as the label, seeing as the other variables where considered labels by the assignment, and we're interested in the sterol type in the membrane. Looking at the PCA plot and the heatmap it would seem like some of the variables are heavily correlated. However, it was decided not to remove any of the variables as of this time.

Here is a PCA plot that is made using the data after applying log and cube transformations to reduce skewness.
```{r PCA3}
# Create PCA plot
new_help <- log10(correlation[2:6])
new_help$APL <- correlation$APL^2
apply(new_help, 2, skewness, na.rm =TRUE)
res.pca <- prcomp(new_help, scale = TRUE)
plot1 <- fviz_eig(res.pca) + 
  theme(axis.title.y = element_text(size=6))
plot2 <- fviz_pca_var(res.pca,
                      col.var = "contrib", # Colour by contributions to the PC
                      gradient.cols = c("orange", "purple"),
                      repel = TRUE     # Avoid text overlapping
                      ) 
plot3 <- fviz_pca_biplot(res.pca, repel = FALSE,
                         col.var = "purple", # Variables colour
                         col.ind = nona_data$sterol.type,  # Individuals colour
                         label = FALSE,
                         addEllipses =  TRUE,
                         ellipse.type = "t"
                         )
# Arrange plots
plot <- arrangeGrob(plot1,  plot2,
                    plot3,
                    ncol = 2, nrow = 2, 
                    layout_matrix = rbind(c(1,2), c(3,3)))

plot <- as_ggplot(plot) +
  draw_plot_label(label = c("A", "B", "C"), size = 15,
                  x = c(0, 0.5, 0), y = c(1, 1, 0.5))
title <- expression(atop(bold("Figure 14:"), 
                         scriptstyle("PCA results after transformation")))
annotate_figure(plot,
                top=text_grob(title))
```
As we can see in figure 14, reducing the skewness had absolutely no effect, besides increasing the variance expained by the first dimension and flipping the PCA upside down.


```{r}
kable(head(data), caption="Original data") %>%
  kable_styling(latex_options = c("scale_down", "hold_position"))

kable(head(scaled_data), caption="Cleaned data") %>%
  kable_styling(latex_options = c("scale_down", "hold_position"))
```

## Min Max
This is code to make a csv file containing the minimums and maximums of every column so that we can use it in the java wrapper.

```{r Min Max data}
columns <- c("APL", "thickness", "bending",
             "tilt", "zorder","compress")

min_max1 <- tibble("Column" = columns,
                   "Min" = apply(nona_data[, columns], 2, min),
                   "Max" = apply(nona_data[, columns], 2, max))
write.csv(min_max1, "Min_Max/min_max", row.names = FALSE)
```



# Weka
## Rescaling data
Here we rescale all of the possible labels to factors so when it is exported to a .arff file it will be encoded correctly.
```{r Scaling2}
scale_min_max <- function(x) (x - min(x)) / (max(x) - min(x))
nona_copy <- nona_data

#str(nona_copy)

nona_copy[1:8] <- lapply(nona_copy[1:8], factor) 
str(nona_copy)
nona_copy_scaled <- nona_copy

index <- sapply(nona_copy_scaled, is.numeric)
nona_copy_scaled[index] <- lapply(nona_copy_scaled[index], scale_min_max)

#scaled_data <- as.data.frame(lapply(nona_copy, FUN=scale_min_max))
#hist.data.frame(scaled_data)
#str(nona_copy)
```


Here is a function to put a specific column as the last column for the encoding in the .arff file.
```{r MakeLast}
library(RWeka)
Mutate_df <- function(data, last_wanted_column){
  data %>%
    relocate(all_of(last_wanted_column), .after = last_col()) %>%
    select(tail(names(.), 7)) %>%
    #mutate_at(last_wanted_column, as.factor) %>%
    write.arff(file = paste0("data/scaled_data_", last_wanted_column, ".arff"))
}
Mutate_df(nona_copy_scaled, "tails")
Mutate_df(nona_copy_scaled, "sterol.type")
Mutate_df(nona_copy_scaled, "sterol.conc")

```



Here is some code to make a .arff file.
```{r}
library(RWeka)
write.arff(scaled_data, file = "data/scaled_data.arff")
```

## Quality metrics

The two most important metrics that we'll be using are the accuracy and the confusion matrix. The accuracy is important becuase it tells us how accurate the algorithm is. The confusion matrix is also very important seeing as this can tell us if the model may be prediction some classes with high accuracy while have a very poor accuracy on other classes.

## Performance

The speed of the algorithm isn't of the utmost importance, we would rather have a very accurate model that is slow as hell, then a really fast one at the cost of accuracy. Scalability wise, it should be able to deal with a rather large amount of data. We could use online classification, but it seems like this wouldn't really be productive seeing as the distribution or the classification method won't change over time. With batch classification we keep full control over the data that we use when training, while with online classification we are at risk that due to a feed of bad data that could be entered, which would then change the model. To combat this we would need to filter all of the data that goes in, which is feasible but a lot of work. It would save us resources but this isn't really a concern to start with. Batch learning is also more time intesive than online learning, but this also isn't really a concern.

## Output

### Sterol prediction

Tester:     weka.experiment.PairedCorrectedTTester -G 4,5,6 -D 1 -R 2 -S 0.05 -result-matrix \
"weka.experiment.ResultMatrixLatex -mean-prec 2 -stddev-prec 0 -col-name-width 0 -row-name-width 0 \
-mean-width 2 -stddev-width 2 -sig-width 1 -count-width 0 -print-row-names -enum-col-names"\
Analysing:  Percent_correct\
Datasets:   1\
Resultsets: 9\
Confidence: 0.05 (two tailed)\
Sorted by:  -\
Date:       10/10/21, 4:18 PM\


\begin{table}[thb]
\caption{\label{table:sterol_accuracy}Sterol prediction accuracy}
\footnotesize
{\centering \begin{tabular}{lrr@{\hspace{0.1cm}}cr@{\hspace{0.1cm}}cr@{\hspace{0.1cm}}cr@{\hspace{0.1cm}}cr@{\hspace{0.1cm}}cr@{\hspace{0.1cm}}cr@{\hspace{0.1cm}}cr@{\hspace{0.1cm}}c}
\\
\hline
Dataset & (1)& (2) & & (3) & & (4) & & (5) & & (6) & & (7) & & (8) & & (9) & \\
\hline
R-data-frame & 42.94 & 56.65 & $\circ$ & 59.51 & $\circ$ & 47.83 & $\circ$ & 57.30 & $\circ$ & 60.20 & $\circ$ & 57.07 & $\circ$ & 58.03 & $\circ$ & 70.03 & $\circ$\\
\hline
\multicolumn{18}{c}{$\circ$, $\bullet$ statistically significant improvement or degradation}\\
\end{tabular} \footnotesize \par}
\end{table}


\begin{table}[thb]
\caption{\label{table:sterol_keys}Sterol prediction accuracy (Key)}
\scriptsize
{\centering
\begin{tabularx}{\textwidth}{lX}\\
(1) & rules.ZeroR '' 48055541465867954 \\
(2) & rules.OneR '-B 50' -3459427003147861443 \\
(3) & lazy.IBk '-K 100 -W 0 -A \textbackslash"weka.core.neighboursearch.LinearNNSearch -A \textbackslash\textbackslash\textbackslash"weka.core.EuclideanDistance -R first-last\textbackslash\textbackslash\textbackslash"\textbackslash"' -3080186098777067172 \\
(4) & bayes.NaiveBayes '' 5995231201785697655 \\
(5) & trees.J48 '-C 0.25 -M 200' -217733168393644444 \\
(6) & trees.RandomForest '-P 100 -I 100 -num-slots 1 -K 0 -M 1.0 -V 0.001 -S 1' 1116839470751428698 \\
(7) & functions.SMO '-C 1.0 -L 0.001 -P 1.0E-12 -N 0 -V -1 -W 1 -K \textbackslash"functions.supportVector.PolyKernel -E 1.0 -C 250007\textbackslash" -calibrator \textbackslash"functions.Logistic -R 1.0E-8 -M -1 -num-decimal-places 4\textbackslash"' -6585883636378691736 \\
(8) & functions.SimpleLogistic '-I 0 -M 500 -H 50 -W 0.0' 7397710626304705059 \\
(9) & trees.LMT '-I -1 -M 15 -W 0.0' -1113212459618104943 \\
\end{tabularx}
}
\end{table}


In table \ref{table:sterol_accuracy} we can see the results of specific machine learning models in predicting the sterol type of a given membrane, the settings for every algorithm can be found in \ref{table:sterol_keys}. These aren't the result we where hoping for. It seems like there is no way of descerning between sterol type, seeing as the highest achieved accuracy is equal to around 61%. This is quite poor to say the least. Maybe we should take a closer look at the confusion matrix of some of the algorithms to see if we can find out what's going on here. When we look at the confusion matrices of algorithems like LMT, RandomForest and SimpleLogistic, we get a pretty good idea of what is going on here. The confusion matrices of RandomForest and SimpleLogistic are show in table \ref{table:RandomForest_confus}. It seems like the algorithms are very capable of predicting whether or not there is a sterol present in the membrane or not. They just can't distinguish between ergosterol and cholesterol. We can test this hypothesis even further by removing all of the instances that have no sterol in the membrane. If what we saw here is indeed what is going on, we would expect to see around a 50% accuracy. If this is the case we could change the encoding in the dataset from "no", "ergo" and "chole" to just "no" and combine ergo and chole into "yes". The entire column can then be changed to just "sterol.present".


\begin{table}[thb]
\caption{\label{table:RandomForest_confus}Confusion matrix from RandomForest and SimpleLogistics, sterol type}
\begin{center}
\begin{tabular}{lll}
\begin{tabular}[c]{ c c c|p{3cm} }
 \hline
 \multicolumn{4}{ c }{RandomForest}\\
 \hline
 a & b & c & <-- classified as \\ 
 \hline
 608 & 535 & 0 & a = chole \\  
 \hline
 261 & 893 & 1 & b = ergo \\
 \hline
 1 & 4 & 387 & c = no \\
 \hline
\end{tabular}
&
\begin{tabular}[c]{ c c c|p{3cm} }
 \hline
 \multicolumn{4}{ c }{SimpleLogistics}\\
 \hline
 a & b & c & <-- classified as \\ 
 \hline
 624 & 511 & 8 & a = chole \\  
 \hline
 585 & 564 & 6 & b = ergo \\
 \hline
 18 & 2 & 372 & c = no \\
 \hline
\end{tabular}
\end{tabular}
\end{center}
\end{table}

Here we'll do some quick recoding of the sterol column, and run random forest to see the results. The newly generated dataset will consist of one that has the no sterol column removed and one were ergosterol and cholesterol are recoded to "Present". The data structure will be printed as a way of verifying whether or not everything right.

```{r Sterol recoding}
Mutate_df2 <- function(data, last_wanted_column, removed){
  data %>%
    relocate(all_of(last_wanted_column), .after = last_col()) %>%
    select(tail(names(.), 7)) %>%
    #mutate_at(last_wanted_column, as.factor) %>%
    write.arff(file = paste0("data/scaled_data_recoded_", last_wanted_column, removed, ".arff"))
}

Sterol_present <- nona_copy_scaled
names(Sterol_present)[names(Sterol_present) == "sterol.type"] <- "sterol.present"
Sterol_present$sterol.present <- Sterol_present$sterol.present %>% fct_collapse(yes = c("ergo","chole"))
Sterol_no_removed <- filter(nona_copy_scaled, `sterol.type` != "no")

str(Sterol_present)
str(Sterol_no_removed)

Mutate_df2(Sterol_present, "sterol.present", "")
Mutate_df2(Sterol_no_removed, "sterol.type", "_removed_no")
```

Everything seems to have gone right with the encoding. Now lets run experiment 1 again on the new files. Before running the experiment the \@relation in both files were changed to "Sterol_Present" in scaled_data_recoded_sterol.present.arff and "Removed_No" in scaled_data_recoded_sterol.type_removed_no.arff. This is because the write.arff function does not correctly give a name to the given file. The relation argument also doesn't seem to work correctly.


Tester:     weka.experiment.PairedCorrectedTTester -G 4,5,6 -D 1 -R 2 -S 0.05\
-result-matrix "weka.experiment.ResultMatrixLatex -mean-prec 2 -stddev-prec 2\
-col-name-width 0 -row-name-width 0 -mean-width 0 -stddev-width 0 -sig-width 0\
-count-width 0 -print-row-names -enum-col-names"\
Analysing:  Percent_correct\
Datasets:   2\
Resultsets: 9\
Confidence: 0.05 (two tailed)\
Sorted by:  -\
Date:       10/29/21, 5:10 PM\


\begin{table}[thb]
\caption{\label{table:recode_sterol}Recoded sterol type prediction accuracy}
\footnotesize
{\centering \begin{tabular}{lrr@{\hspace{0.1cm}}cr@{\hspace{0.1cm}}cr@{\hspace{0.1cm}}cr@{\hspace{0.1cm}}cr@{\hspace{0.1cm}}cr@{\hspace{0.1cm}}cr@{\hspace{0.1cm}}cr@{\hspace{0.1cm}}c}
\\
\hline
Dataset & (6)& (1) & & (2) & & (3) & & (4) & & (5) & & (7) & & (8) & & (9) & \\
\hline
Sterol-Present & 99.75 & 85.43 & $\bullet$ & 93.36 & $\bullet$ & 92.93 & $\bullet$ & 86.91 & $\bullet$ & 92.78 & $\bullet$ & 90.35 & $\bullet$ & 98.99 & $\bullet$ & 99.94 &        \\
Removed-No & 53.30 & 50.26 & $\bullet$ & 55.67 &           & 55.06 &           & 53.88 &           & 57.31 &   $\circ$ & 51.43 &           & 52.90 &           & 57.54 & $\circ$\\
\hline
\multicolumn{18}{c}{$\circ$, $\bullet$ statistically significant improvement or degradation}\\
\end{tabular} \footnotesize \par}
\end{table}


After the recording we find that we can get a staggering 99.75% percent accuracy when predicting whether or not a sterol is present in the membrane using the RandomForest algorithm. But then when looking at specifically distinguishing between ergosterol and cholesterol we only get a measly 57.54% accuracy as the highest accuracy. These results are visible in \ref{table:recode_sterol, the keys are the same as we saw in \ref{table:recode_sterol}.
What we have inadvertently done here is created evidence to support the theory that there in no significant difference in the effect of cholesterol compared to ergosterol on membrane characteristics. We could choose to pursue this even further, but that isn't really the goal of this project. 
Because of these findings I have decided to use the RandomForest algorithm, seeing as is it significantly better than every other algorithm except for LMT. I will use the model that predicts whether or not a sterol is present in the membrane, and not what specific sterol it is. This is because as we could see in the results, predicting the specific sterol type in a membrane gives absolutely awful results. There is however, one big drawback to this. This being that the dataset now contains 2298 instances with a sterol present and only 392 that don't contain a sterol in the membrane.

### Tails prediction

#### Tails Test 1
Testing a multitude of machine learning algorithms.

Tester:     weka.experiment.PairedCorrectedTTester -G 4,5,6 -D 1 -R 2 -S 0.05 -result-matrix \
"weka.experiment.ResultMatrixLatex -mean-prec 2 -stddev-prec 0 -col-name-width 0 -row-name-width 0 \
-mean-width 2 -stddev-width 2 -sig-width 1 -count-width 0 -print-row-names -enum-col-names"\
Analysing:  Percent_correct  
Datasets:   1  
Resultsets: 9  
Confidence: 0.05 (two tailed)  
Sorted by:  -  
Date:       10/10/21, 4:26 PM  


\begin{table}[thb]
\caption{\label{table:tails_Accuracy}Accuracy tails prediction}
\footnotesize
{\centering \begin{tabular}{lrr@{\hspace{0.1cm}}cr@{\hspace{0.1cm}}cr@{\hspace{0.1cm}}cr@{\hspace{0.1cm}}cr@{\hspace{0.1cm}}cr@{\hspace{0.1cm}}cr@{\hspace{0.1cm}}cr@{\hspace{0.1cm}}c}
\\
\hline
Dataset & (1)& (2) & & (3) & & (4) & & (5) & & (6) & & (7) & & (8) & & (9) & \\
\hline
R-data-frame & 37.81 & 82.28 & $\circ$ & 82.39 & $\circ$ & 77.83 & $\circ$ & 81.03 & $\circ$ & 99.47 & $\circ$ & 79.71 & $\circ$ & 99.42 & $\circ$ & 99.94 & $\circ$\\
\hline
\multicolumn{18}{c}{$\circ$, $\bullet$ statistically significant improvement or degradation}\\
\end{tabular} \footnotesize \par}
\end{table}

\begin{table}[thb]
\caption{\label{table:tails_keys}Accuracy tails prediction (Key)}
\scriptsize
{\centering
\begin{tabularx}{\textwidth}{lX}\\
(1) & rules.ZeroR '' 48055541465867954 \\
(2) & rules.OneR '-B 50' -3459427003147861443 \\
(3) & lazy.IBk '-K 100 -W 0 -A \textbackslash"weka.core.neighboursearch.LinearNNSearch -A \textbackslash\textbackslash\textbackslash"weka.core.EuclideanDistance -R first-last\textbackslash\textbackslash\textbackslash"\textbackslash"' -3080186098777067172 \\
(4) & bayes.NaiveBayes '' 5995231201785697655 \\
(5) & trees.J48 '-C 0.25 -M 200' -217733168393644444 \\
(6) & trees.RandomForest '-P 100 -I 100 -num-slots 1 -K 0 -M 1.0 -V 0.001 -S 1' 1116839470751428698 \\
(7) & functions.SMO '-C 1.0 -L 0.001 -P 1.0E-12 -N 0 -V -1 -W 1 -K \textbackslash"functions.supportVector.PolyKernel -E 1.0 -C 250007\textbackslash" -calibrator \textbackslash"functions.Logistic -R 1.0E-8 -M -1 -num-decimal-places 4\textbackslash"' -6585883636378691736 \\
(8) & functions.SimpleLogistic '-I 0 -M 500 -H 50 -W 0.0' 7397710626304705059 \\
(9) & trees.LMT '-I -1 -M 15 -W 0.0' -1113212459618104943 \\
\end{tabularx}
}
\end{table}

The results can be found in table \ref{table:tails_Accuracy}, the keys/commands for every algorithm can be found in table \ref{table:tails_keys}. It looks like RandomForest and LMT score very high. Lets set RandomForest as our base now to compare which is significantly better.


#### Test 2
Testing the best algortithms in ensemble learners.

Tester:     weka.experiment.PairedCorrectedTTester -G 4,5,6 -D 1 -R 2 -S 0.05 -result-matrix \
"weka.experiment.ResultMatrixLatex -mean-prec 2 -stddev-prec 2 -col-name-width 0 \
-row-name-width 0 -mean-width 0 -stddev-width 0 -sig-width 0 -count-width 0 -print-row-names \
-enum-col-names"\
Analysing:  Percent_correct\
Datasets:   1\
Resultsets: 9\
Confidence: 0.05 (two tailed)\
Sorted by:  -\
Date:       12-10-21 12:48\


\begin{table}[thb]
\caption{\label{table:RF_Tails}Tails prediction, RandomForest as base of t test}
\footnotesize
{\centering \begin{tabular}{lrr@{\hspace{0.1cm}}cr@{\hspace{0.1cm}}cr@{\hspace{0.1cm}}cr@{\hspace{0.1cm}}cr@{\hspace{0.1cm}}cr@{\hspace{0.1cm}}cr@{\hspace{0.1cm}}cr@{\hspace{0.1cm}}c}
\\
\hline
Dataset & (6)& (1) & & (2) & & (3) & & (4) & & (5) & & (7) & & (8) & & (9) & \\
\hline
R-data-frame & 99.47 & 37.81 & $\bullet$ & 82.28 & $\bullet$ & 82.39 & $\bullet$ & 77.83 & $\bullet$ & 81.03 & $\bullet$ & 79.71 & $\bullet$ & 99.42 &         & 99.94 & $\circ$\\
\hline
\multicolumn{18}{c}{$\circ$, $\bullet$ statistically significant improvement or degradation}\\
\end{tabular} \footnotesize \par}
\end{table}


\begin{table}[thb]
\caption{\label{table:RF_Tails_keys}Tails prediction, RandomForest base (Key)}
\scriptsize
{\centering
\begin{tabularx}{\textwidth}{lX}\\
(1) & rules.ZeroR '' 48055541465867954 \\
(2) & rules.OneR '-B 50' -3459427003147861443 \\
(3) & lazy.IBk '-K 100 -W 0 -A \textbackslash"weka.core.neighboursearch.LinearNNSearch -A \textbackslash\textbackslash\textbackslash"weka.core.EuclideanDistance -R first-last\textbackslash\textbackslash\textbackslash"\textbackslash"' -3080186098777067172 \\
(4) & bayes.NaiveBayes '' 5995231201785697655 \\
(5) & trees.J48 '-C 0.25 -M 200' -217733168393644444 \\
(6) & trees.RandomForest '-P 100 -I 100 -num-slots 1 -K 0 -M 1.0 -V 0.001 -S 1' 1116839470751428698 \\
(7) & functions.SMO '-C 1.0 -L 0.001 -P 1.0E-12 -N 0 -V -1 -W 1 -K \textbackslash"functions.supportVector.PolyKernel -E 1.0 -C 250007\textbackslash" -calibrator \textbackslash"functions.Logistic -R 1.0E-8 -M -1 -num-decimal-places 4\textbackslash"' -6585883636378691736 \\
(8) & functions.SimpleLogistic '-I 0 -M 500 -H 50 -W 0.0' 7397710626304705059 \\
(9) & trees.LMT '-I -1 -M 15 -W 0.0' -1113212459618104943 \\
\end{tabularx}
}
\end{table}

Table \ref{table:RF_Tails} shows that RandomForest is significantly better than most of the algorithms. However, LMT is significantly better, and SimpleLogistic isn't significantly better nor significantly worse. Even though LMT seems to be the best at the moment, I don't think I'll use it. This is for the simple reason that I do not have enough knowledge on the algorithm to understand how it works, which means I do not have enough confidence in using it. It's also understandable that RandomForest came does quite well, seeing as it is technically a form of ensemble learning.

#### Enseble Learning 
Testing ensemble learning algorithms with the best scoring algorithms.\

Tester:     weka.experiment.PairedCorrectedTTester -G 4,5,6 -D 1 -R 2 -S 0.05 -result-matrix \
"weka.experiment.ResultMatrixLatex -mean-prec 2 -stddev-prec 0 -col-name-width 0 \
-row-name-width 0 -mean-width 0 -stddev-width 0 -sig-width 0 -count-width 0 -print-row-names \
-enum-col-names"\
Analysing:  Percent_correct\
Datasets:   1\
Resultsets: 6\
Confidence: 0.05 (two tailed)\
Sorted by:  -\
Date:       12-10-21 14:41\


\begin{table}[thb]
\caption{\label{table:Ens_tails}Ensemble learning tails prediction accuracy}
\footnotesize
{\centering \begin{tabular}{lrr@{\hspace{0.1cm}}cr@{\hspace{0.1cm}}cr@{\hspace{0.1cm}}cr@{\hspace{0.1cm}}cr@{\hspace{0.1cm}}c}
\\
\hline
Dataset & (1)& (2) & & (3) & & (4) & & (5) & & (6) & \\
\hline
R-data-frame & 99.47 & 99.41 &         & 99.69 &         & 94.41 & $\bullet$ & 99.29 &         & 99.48 &        \\
\hline
\multicolumn{12}{c}{$\circ$, $\bullet$ statistically significant improvement or degradation}\\
\end{tabular} \footnotesize \par}
\end{table}


\begin{table}[thb]
\caption{\label{table:Ens_tails_keys}Ensemble learning tail prediction (Key)}
\scriptsize
{\centering
\begin{tabularx}{\textwidth}{lX}\\
(1) & trees.RandomForest '-P 100 -I 100 -num-slots 1 -K 0 -M 1.0 -V 0.001 -S 1' 1116839470751428698 \\
(2) & meta.Vote '-S 1 -B \textbackslash"trees.RandomForest -P 100 -I 100 -num-slots 1 -K 0 -M 1.0 -V 0.001 -S 1\textbackslash" -B \textbackslash"functions.SimpleLogistic -I 0 -M 500 -H 50 -W 0.0\textbackslash" -B \textbackslash"lazy.IBk -K 100 -W 0 -A \textbackslash\textbackslash\textbackslash"weka.core.neighboursearch.LinearNNSearch -A \textbackslash\textbackslash\textbackslash\textbackslash\textbackslash\textbackslash\textbackslash"weka.core.EuclideanDistance -R first-last\textbackslash\textbackslash\textbackslash\textbackslash\textbackslash\textbackslash\textbackslash"\textbackslash\textbackslash\textbackslash"\textbackslash" -R AVG' -637891196294399624 \\
(3) & meta.Stacking '-X 10 -M \textbackslash"trees.J48 -C 0.25 -M 2\textbackslash" -S 1 -num-slots 1 -B \textbackslash"trees.RandomForest -P 100 -I 100 -num-slots 1 -K 0 -M 1.0 -V 0.001 -S 1\textbackslash" -B \textbackslash"functions.SimpleLogistic -I 0 -M 500 -H 50 -W 0.0\textbackslash" -B \textbackslash"lazy.IBk -K 100 -W 0 -A \textbackslash\textbackslash\textbackslash"weka.core.neighboursearch.LinearNNSearch -A \textbackslash\textbackslash\textbackslash\textbackslash\textbackslash\textbackslash\textbackslash"weka.core.EuclideanDistance -R first-last\textbackslash\textbackslash\textbackslash\textbackslash\textbackslash\textbackslash\textbackslash"\textbackslash\textbackslash\textbackslash"\textbackslash"' 5134738557155845452 \\
(4) & meta.MultiClassClassifier '-M 0 -R 2.0 -S 1 -W functions.Logistic -- -R 1.0E-8 -M -1 -num-decimal-places 4' -3879602011542849141 \\
(5) & meta.Bagging '-P 100 -S 1 -num-slots 1 -I 10 -W trees.RandomForest -- -P 100 -I 100 -num-slots 1 -K 0 -M 1.0 -V 0.001 -S 1' -115879962237199703 \\
(6) & meta.AdaBoostM1 '-P 100 -S 1 -I 10 -W trees.RandomForest -- -P 100 -I 100 -num-slots 1 -K 0 -M 1.0 -V 0.001 -S 1' -1178107808933117974 \\
\end{tabularx}
}
\end{table}

Here we look at the area under the curve for all of the algoritms to check for a difference, the results are shown in table\ref{table:Ens_tails}. It uses the same algorithms/keys as in table \ref{table:RF_Tails_keys}, also shown in table \ref{table:Ens_tails_keys} for convenience.

These are the results for the area under the curve for the same experiment.
Tester:     weka.experiment.PairedCorrectedTTester -G 4,5,6 -D 1 -R 2 -S 0.05 -V \
-result-matrix "weka.experiment.ResultMatrixLatex -mean-prec 2 -stddev-prec 0 \
-col-name-width 0 -row-name-width 0 -mean-width 0 -stddev-width 0 -sig-width 0 \
-count-width 0 -show-stddev -print-row-names -enum-col-names"\
Analysing:  Area_under_ROC\
Datasets:   1\
Resultsets: 6\
Confidence: 0.05 (two tailed)\
Sorted by:  -\
Date:       13-10-21 13:39\


\begin{table}[thb]
\caption{\label{table:AUC_tail}AUC, tail predictions}
\scriptsize
{\centering \begin{tabular}{lr@{\hspace{0cm}}c@{\hspace{0cm}}rr@{\hspace{0cm}}c@{\hspace{0cm}}r@{\hspace{0.1cm}}cr@{\hspace{0cm}}c@{\hspace{0cm}}r@{\hspace{0.1cm}}cr@{\hspace{0cm}}c@{\hspace{0cm}}r@{\hspace{0.1cm}}cr@{\hspace{0cm}}c@{\hspace{0cm}}r@{\hspace{0.1cm}}cr@{\hspace{0cm}}c@{\hspace{0cm}}r@{\hspace{0.1cm}}c}
\\
\hline
Dataset & \multicolumn{3}{c}{(1)}& \multicolumn{4}{c}{(2)} & \multicolumn{4}{c}{(3)} & \multicolumn{4}{c}{(4)} & \multicolumn{4}{c}{(5)} & \multicolumn{4}{c}{(6)} \\
\hline
R-data-frame & 1.00 & $\pm$ & 0 & 1.00 & $\pm$ & 0 &         & 1.00 & $\pm$ & 0 &         & 1.00 & $\pm$ & 0 &         & 1.00 & $\pm$ & 0 &         & 1.00 & $\pm$ & 0 &        \\
\hline
\multicolumn{18}{c}{$\circ$, $\bullet$ statistically significant improvement or degradation}\\
\end{tabular} \scriptsize \par}
\end{table}

These results shown in table \ref{table:AUC_tai} are quit weird. All of the AUC (Area Under the Curve) values are 1.00 with a standard deviation of 0. Which is quite interesting, seeing as the accuracy isn't 100%. This probably has to do with the fact that there are 5 different labels, of which some can be predicted with a 100% accuracy. It probably chooses the first label as positive, which in this case would be DI, which indeed has an area under the curve of 1.

#### Attribute selection
These are the result of attribute selection on the random forest algorithm.

Tester:     weka.experiment.PairedCorrectedTTester -G 4,5,6 -D 1 -R 2 -S 0.05 \
-result-matrix "weka.experiment.ResultMatrixLatex -mean-prec 2 -stddev-prec 2 \
-col-name-width 0 -row-name-width 0 -mean-width 0 -stddev-width 0 -sig-width 0 \
-count-width 0 -print-row-names -enum-col-names"\
Analysing:  Percent_correct\
Datasets:   1\
Resultsets: 2\
Confidence: 0.05 (two tailed)\
Sorted by:  -\
Date:       10/25/21, 6:15 PM\


\begin{table}[thb]
\caption{\label{table:att_ac}Attribute selection accuracy}
\footnotesize
{\centering \begin{tabular}{lrr@{\hspace{0.1cm}}c}
\\
\hline
Dataset & (1)& (2) & \\
\hline
R-data-frame & 99.47 & 99.54 &        \\
\hline
\multicolumn{4}{c}{$\circ$, $\bullet$ statistically significant improvement or degradation}\\
\end{tabular} \footnotesize \par}
\end{table}


\begin{table}[thb]
\caption{\label{table:att_ac_key}Attribute selection accuracy (Key)}
\scriptsize
{\centering
\begin{tabularx}{\textwidth}{lX}\\
(1) & trees.RandomForest '-P 100 -I 100 -num-slots 1 -K 0 -M 1.0 -V 0.001 -S 1' 1116839470751428698 \\
(2) & meta.AttributeSelectedClassifier '-E \textbackslash"WrapperSubsetEval -B trees.RandomForest -F 5 -T 0.01 -R 1 -E DEFAULT -- -P 100 -I 100 -num-slots 1 -K 0 -M 1.0 -V 0.001 -S 1\textbackslash" -S \textbackslash"BestFirst -D 1 -N 5\textbackslash" -W trees.RandomForest -- -P 100 -I 100 -num-slots 1 -K 0 -M 1.0 -V 0.001 -S 1' -1151805453487947577 \\
\end{tabularx}
}
\end{table}

As we can see in table \ref{table:att_ac}, attribute selection doesn't make a significant difference in results. This would lead to the conclusion that it is probably best to run the model with less attributes, seeing as less input would be better. However, for the final program it isn't actually that useful to use less attributes seeing as we'll be using a multitude of models to predict multiple components of the given membrane. Most of these models use different attributes, so we'll probably still need all of them as input for our program.

#### A comment on paramater optimization
I would love to be able to do some paramater optimization on the LMT and RandomForest algorithms. However, whenever I try to do this, Weka won't let me. It constantly gives errors, which is the reason why there isn't a complete subchapter on this subject.


#### ROC curve
Here we'll plot the ROC curve of the RandomForest algorithm.
```{r}
#library(pROC)

AUC_calculator <- function(TPR, FPR){
  dFPR <- c(diff(FPR), 0)
  dTPR <- c(diff(TPR), 0)
  abs(sum(TPR * dFPR) + sum(dTPR * dFPR)/2)
}

ROC_variable_maker <- function(filehandle){
  open_file <- RWeka::read.arff(filehandle)
  AUC <- with(open_file, AUC_calculator(`True Positive Rate`, `False Positive Rate`))
  #print(head(open_file))
  plot <- ggplot(open_file)+ 
    geom_path(aes(x=`False Positive Rate`,
                  y=`True Positive Rate`,
                  color=Threshold)) +
    scale_color_gradient2(low="orange", mid="mediumvioletred",
                          high="purple", midpoint = 0.5) +
    labs(title= paste("ROC curve, AUC: ", round(AUC, 6)),
         x = "False Positive Rate (1-Specificity)",
         y = "True Positive Rate (Sensitivity)") + 
    theme(plot.title = element_text(size=10),
          axis.title.x=element_text(size=9),
          axis.title.y=element_text(size=7),   
          legend.title = element_text(size = 8),
          legend.text = element_text(size = 6))
  return(plot)
  
}

ROC_creator <- function (filefolder){
  files <- list.files(path=filefolder, pattern="*.arff", full.names=TRUE, recursive=FALSE)
  #print(files)
  plots <- lapply(files, ROC_variable_maker)
  
  n <- length(plots)
  nCols <- floor(sqrt(n))
  do.call("grid.arrange", c(plots, ncol= nCols))
}

ROC_creator("./Experiment/Results_Explorer/ROC_Curve")
```
All of the ROC curves look extremely good seeing as it's basically they are all at what is basically a right angle. We can also see this in the area under the curve, which is close to one for all for all of the different labels.

### Sterol concentration prediction
This section will be a bit less extensive than the tails prediction because of certain time restraints.
The keys are again the exact same ones shown in table 10  because the same experiment was used.


#### Experiment 1
These are the results of running the experiment 1 configuration on the dataset with sterol concentration on as the last column.

Tester:     weka.experiment.PairedCorrectedTTester -G 4,5,6 -D 1 -R 2 -S 0.05 -result-matrix \
"weka.experiment.ResultMatrixLatex -mean-prec 2 -stddev-prec 0 -col-name-width 0 -row-name-width 0 \
-mean-width 0 -stddev-width 0 -sig-width 0 -count-width 0 -print-row-names -enum-col-names" \
Analysing:  Percent_correct \
Datasets:   1 \
Resultsets: 9 \
Confidence: 0.05 (two tailed) \
Sorted by:  - \
Date:       10/23/21, 2:29 PM \


\begin{table}[thb]
\caption{\label{table:sterol_con_pred}Sterol concentration prediction accuracy}
\footnotesize
{\centering \begin{tabular}{lrr@{\hspace{0.1cm}}cr@{\hspace{0.1cm}}cr@{\hspace{0.1cm}}cr@{\hspace{0.1cm}}cr@{\hspace{0.1cm}}cr@{\hspace{0.1cm}}cr@{\hspace{0.1cm}}cr@{\hspace{0.1cm}}c}
\\
\hline
Dataset & (9)& (1) & & (2) & & (3) & & (4) & & (5) & & (6) & & (7) & & (8) & \\
\hline
R-data-frame & 99.94 & 37.81 & $\bullet$ & 82.28 & $\bullet$ & 82.39 & $\bullet$ & 77.83 & $\bullet$ & 81.03 & $\bullet$ & 99.47 & $\bullet$ & 79.71 & $\bullet$ & 99.42 & $\bullet$\\
\hline
\multicolumn{18}{c}{$\circ$, $\bullet$ statistically significant improvement or degradation}\\
\end{tabular} \footnotesize \par}
\end{table}

\begin{table}[thb]
\caption{\label{table:sterol_con_pred_key}Sterol concentration prediction accuracy (Key)}
\scriptsize
{\centering
\begin{tabularx}{\textwidth}{lX}\\
(1) & rules.ZeroR '' 48055541465867954 \\
(2) & rules.OneR '-B 50' -3459427003147861443 \\
(3) & lazy.IBk '-K 100 -W 0 -A \textbackslash"weka.core.neighboursearch.LinearNNSearch -A \textbackslash\textbackslash\textbackslash"weka.core.EuclideanDistance -R first-last\textbackslash\textbackslash\textbackslash"\textbackslash"' -3080186098777067172 \\
(4) & bayes.NaiveBayes '' 5995231201785697655 \\
(5) & trees.J48 '-C 0.25 -M 200' -217733168393644444 \\
(6) & trees.RandomForest '-P 100 -I 100 -num-slots 1 -K 0 -M 1.0 -V 0.001 -S 1' 1116839470751428698 \\
(7) & functions.SMO '-C 1.0 -L 0.001 -P 1.0E-12 -N 0 -V -1 -W 1 -K \textbackslash"functions.supportVector.PolyKernel -E 1.0 -C 250007\textbackslash" -calibrator \textbackslash"functions.Logistic -R 1.0E-8 -M -1 -num-decimal-places 4\textbackslash"' -6585883636378691736 \\
(8) & functions.SimpleLogistic '-I 0 -M 500 -H 50 -W 0.0' 7397710626304705059 \\
(9) & trees.LMT '-I -1 -M 15 -W 0.0' -1113212459618104943 \\
\end{tabularx}
}
\end{table}

Looking at table \ref{table:sterol_con_pred}, it seems like the LMT tree gives significantly better results then any of the other models. Let's take a look whether or not we can get some sort of improvements with ensemble learning. Here I will use the LMT tree, seeing as I now have a better understanding of it due to some papers. However, due to time restraints, I am unable to redo all of the experiments for the tails prediction and make a new model. 
It is also important to note that the minimum number of instances was set to 100 in a separate run to try and prevent overfitting. Funnily enough, this didn't change the accuracy of the model at all. The model at a minimum number of instances is the exact same as the one at 100. The model also doesn't seem to be overfitted at 15 seeing as these results are from 10 fold cross-validation and the tree doesn't look to complex. The resulting tree is printed below to illustrate that the tree is indeed not very complex, which would be a sign of overfitting.

=== Classifier model (full training set) ===

Logistic model tree 
------------------

bending <= 0.054892: LM_1:116/232 (215)
bending > 0.054892
|   thickness <= 0.894825
|   |   compress <= 0.089242
|   |   |   APL <= 0.578692: LM_2:116/580 (894)
|   |   |   APL > 0.578692: LM_3:116/580 (1029)
|   |   compress > 0.089242: LM_4:116/464 (384)
|   thickness > 0.894825: LM_5:116/348 (168)

Number of Leaves  : 	5

Size of the Tree : 	9

#### Ensemble learners (Experiment 4)
Here are the results of making ensemble learners using the best performing algorithms.

Tester:     weka.experiment.PairedCorrectedTTester -G 4,5,6 -D 1 -R 2 -S 0.05 \
-result-matrix "weka.experiment.ResultMatrixLatex -mean-prec 2 -stddev-prec 0 \
-col-name-width 0 -row-name-width 0 -mean-width 0 -stddev-width 0 -sig-width 0 \
-count-width 0 -print-row-names -enum-col-names" \
Analysing:  Percent_correct \
Datasets:   1 \
Resultsets: 6 \
Confidence: 0.05 (two tailed) \
Sorted by:  - \
Date:       10/23/21, 7:29 PM \


\begin{table}[thb]
\caption{\label{table:Ens_Ste_conc}Ensemble learning sterol concentration prediction accuracy}
\footnotesize
{\centering \begin{tabular}{lrr@{\hspace{0.1cm}}cr@{\hspace{0.1cm}}cr@{\hspace{0.1cm}}cr@{\hspace{0.1cm}}cr@{\hspace{0.1cm}}c}
\\
\hline
Dataset & (1)& (2) & & (3) & & (4) & & (5) & & (6) & \\
\hline
R-data-frame & 99.73 & 99.64 &         & 99.80 &         & 99.70 &         & 99.72 &         & 36.69 & $\bullet$\\
\hline
\multicolumn{12}{c}{$\circ$, $\bullet$ statistically significant improvement or degradation}\\
\end{tabular} \footnotesize \par}
\end{table}


\begin{table}[thb]
\caption{\label{table:Ens_Ste_conc_keys}Ensemble learning sterol concentration prediction accuracy (Key)}
\scriptsize
{\centering
\begin{tabularx}{\textwidth}{lX}\\
(1) & trees.LMT '-I -1 -M 15 -W 0.0' -1113212459618104943 \\
(2) & trees.RandomForest '-P 100 -I 100 -num-slots 1 -K 0 -M 1.0 -V 0.001 -S 1' 1116839470751428698 \\
(3) & meta.Vote '-S 1 -B \textbackslash"trees.RandomForest -P 100 -I 100 -num-slots 1 -K 0 -M 1.0 -V 0.001 -S 1\textbackslash" -B \textbackslash"trees.LMT -I -1 -M 15 -W 0.0\textbackslash" -B \textbackslash"functions.SMO -C 1.0 -L 0.001 -P 1.0E-12 -N 0 -V -1 -W 1 -K \textbackslash\textbackslash\textbackslash"functions.supportVector.PolyKernel -E 1.0 -C 250007\textbackslash\textbackslash\textbackslash" -calibrator \textbackslash\textbackslash\textbackslash"functions.Logistic -R 1.0E-8 -M -1 -num-decimal-places 4\textbackslash\textbackslash\textbackslash"\textbackslash" -R AVG' -637891196294399624 \\
(4) & meta.Stacking '-X 10 -M \textbackslash"trees.J48 -C 0.25 -M 2\textbackslash" -S 1 -num-slots 1 -B \textbackslash"trees.RandomForest -P 100 -I 100 -num-slots 1 -K 0 -M 1.0 -V 0.001 -S 1\textbackslash" -B \textbackslash"trees.LMT -I -1 -M 15 -W 0.0\textbackslash" -B \textbackslash"functions.SMO -C 1.0 -L 0.001 -P 1.0E-12 -N 0 -V -1 -W 1 -K \textbackslash\textbackslash\textbackslash"functions.supportVector.PolyKernel -E 1.0 -C 250007\textbackslash\textbackslash\textbackslash" -calibrator \textbackslash\textbackslash\textbackslash"functions.Logistic -R 1.0E-8 -M -1 -num-decimal-places 4\textbackslash\textbackslash\textbackslash"\textbackslash"' 5134738557155845452 \\
(5) & meta.Bagging '-P 100 -S 1 -num-slots 1 -I 10 -W trees.LMT -- -I -1 -M 15 -W 0.0' -115879962237199703 \\
(6) & meta.AdaBoostM1 '-P 100 -S 1 -I 10 -W trees.DecisionStump' -1178107808933117974 \\
\end{tabularx}
}
\end{table}

Now this is where stuff gets really interesting. The results of the experiment can be found in table \ref{table:Ens_Ste_conc}, the keys for the experiment can be found in table \ref{table:Ens_Ste_conc_keys}. No algorithm is significantly better than just LMT, except for adaboost, which preforms horrendously bad. But now, for some reason, it also shows that RandomForest does not actually perform significantly worse than LMT. These result are contradictory to what we found in the previous experiment.


## Temporary results
After running a multitude of test in weka to try and make a model to predict sterol type, I found something quite interesting. It seems that it is almost impossible to predict the sterol type in a membrane. By this I mean it is very hard to distinguish between ergosterol and cholesterol. It is however completely possible to predict whether or not there is a sterol present in the membrane. After running some test with the all of the instances with no sterol removed we see that almost every algorithm go's to 50% accuracy, meaning it's basically as good as zeroR. Even when using voting with OneR, ZeroR, RandomForest, J48, NaiveBayes, SimpleLogistic and SMO, we still only get 55.7006 % correctly classified instances (with the no sterol group removed). This gives us an interesting conclusion to the temporary research question. No, it is not possible to predict the sterol in a membrane given certain measurements. It is however possible to distinguish between no sterol and a sterol. This leads me to conclude that there is just very little to no difference between ergosterol and cholesterol, seeing as when testing to predict the sterol concentration we get a 99.1297% using the voting of the same algorithms as used on sterol type (again on the data with instances with no sterol removed), and only given the measurements.



# EDA Discussion
I need to make this very clear.
The EDA was made with a specific question in mind. This probably exposed the exploration steps to an subconscious bias when making plots and exploring relations between variables.


# Sources

https://link.springer.com/content/pdf/10.1007/s10994-005-0466-3.pdf

Czub, J., & Baginski, M. (2006). Comparative molecular dynamics study of lipid membranes containing cholesterol and ergosterol. Biophysical journal, 90(7), 2368-2382.

Hung, W. C., Lee, M. T., Chung, H., Sun, Y. T., Chen, H., Charron, N. E., & Huang, H. W. (2016). Comparative study of the condensing effects of ergosterol and cholesterol. Biophysical journal, 110(9), 2026-2033.
